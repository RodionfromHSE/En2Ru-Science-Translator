{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = {\n",
    "    'dataset': 'saier/unarxive_citrec',\n",
    "    'n_train': 10_000,\n",
    "    'n_valid': 1_000,\n",
    "    'n_test': 1_000,\n",
    "    'max_chars_len': 512,\n",
    "    'min_chars_len': 128,\n",
    "    'save_dir': '../../data/raw/unarxive_citrec/'\n",
    "}\n",
    "config = OmegaConf.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:15<00:00, 664.22it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 877.92it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 754.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset = load_dataset(config.dataset, split='train', streaming=True)\n",
    "\n",
    "def take_n_samples(n: int, split: str, batch_size: int = 250) -> list:\n",
    "    dataset = load_dataset(config.dataset, split=split, streaming=True)\n",
    "    samples = []\n",
    "    bar = tqdm(total=n)\n",
    "    for sample in dataset:\n",
    "        if config.min_chars_len <= len(sample['text']) <= config.max_chars_len:\n",
    "            samples.append(sample)\n",
    "            bar.update(1)\n",
    "        if len(samples) == n:\n",
    "            break\n",
    "\n",
    "    return samples\n",
    "\n",
    "train_samples = take_n_samples(config.n_train, split='train')\n",
    "valid_samples = take_n_samples(config.n_valid, split='validation')\n",
    "test_samples = take_n_samples(config.n_test, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Train samples: {len(train_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 10000\n"
     ]
    }
   ],
   "source": [
    "def extract_texts(samples):\n",
    "    return [sample['text'] for sample in samples]\n",
    "\n",
    "train_texts = extract_texts(train_samples)\n",
    "print(f'Train texts: {len(train_texts)}')\n",
    "valid_texts = extract_texts(valid_samples)\n",
    "test_texts = extract_texts(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9178"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: 72\n",
      "Overlap: 24\n",
      "Overlap: 65\n",
      "Train: 9082, Valid: 702, Test: 568\n"
     ]
    }
   ],
   "source": [
    "def remove_overlap(texts_a, texts_b):\n",
    "    overlap = list(set(texts_a) & set(texts_b))\n",
    "    print(f'Overlap: {len(overlap)}')\n",
    "\n",
    "    # remove\n",
    "    texts_a = list(set(texts_a) - set(overlap))\n",
    "    texts_b = list(set(texts_b) - set(overlap))\n",
    "    return texts_a, texts_b\n",
    "\n",
    "train_texts, valid_texts = remove_overlap(train_texts, valid_texts)\n",
    "train_texts, test_texts = remove_overlap(train_texts, test_texts)\n",
    "valid_texts, test_texts = remove_overlap(valid_texts, test_texts)\n",
    "\n",
    "print(f\"Train: {len(train_texts)}, Valid: {len(valid_texts)}, Test: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that texts don't overlap\n",
    "assert len(set(train_texts) & set(valid_texts)) == 0\n",
    "assert len(set(train_texts) & set(test_texts)) == 0\n",
    "assert len(set(valid_texts) & set(test_texts)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25, 0.5, 0.75 quantile: [264. 358. 439.]\n",
      "Max len: 512\n",
      "Min len: 128\n",
      "Example: Regarding the computational complexity of the algorithm, line REF  is\n",
      "computed in \\(\\mathcal {O}(|V(G)| + |E(G)|)\\)  time [1]}.\n",
      "Moreover, the complete bipartite decomposition of \\(G\\)  is computed\n",
      "in \\(\\mathcal {O}(|V(G)|)\\)  time [2]}. To conclude,\n",
      "the paths at lines REF , REF , REF  and REF \n",
      "are computed in constant time, as each \\(K_i\\)  is complete bipartite. Therefore,\n",
      "the complexity of Algorithm REF  is \\(\\mathcal {O}(|V(G)| + |E(G)|)\\) .\n",
      "\\(\\Box \\)\n",
      "<FIGURE>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_lens = np.array([len(text) for text in train_texts])\n",
    "\n",
    "# 0.25 quantile, 0.5 quantile, 0.75 quantile\n",
    "print(\"0.25, 0.5, 0.75 quantile:\", np.quantile(train_lens, [0.25, 0.5, 0.75]))\n",
    "print(\"Max len:\", np.max(train_lens))\n",
    "print(\"Min len:\", np.min(train_lens))\n",
    "print(\"Example:\", train_texts[np.random.randint(0, len(train_texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(config.save_dir, exist_ok=True)\n",
    "for texts, split_name in [\n",
    "    (train_texts, 'train'),\n",
    "    (valid_texts, 'valid'),\n",
    "    (test_texts, 'test')\n",
    "]:\n",
    "    path = os.path.join(config.save_dir, split_name + '.json')\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(texts, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
